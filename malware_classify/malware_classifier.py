import pandas as pd
import numpy as np
import pickle
from tf_keras.models import Sequential
from tf_keras.layers import Dense
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
import xgboost as xgb

# Load the dataset
malware_dataset = pd.read_csv("C:/Users/Roni/Desktop/P5-Hardwall/MalwareClassifier/MalwareData.csv", sep="|")

# Separate benign and malicious samples
benign_samples = malware_dataset[0:41323].drop(["legitimate"], axis=1)
malicious_samples = malware_dataset[41323::].drop(["legitimate"], axis=1)

print("Shape benign: %s samples, %s features" % (benign_samples.shape[0], benign_samples.shape[1]))
print("Shape malicious: %s samples, %s features" % (malicious_samples.shape[0], malicious_samples.shape[1]))

# Prepare features and labels
raw_features = malware_dataset.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = malware_dataset['legitimate'].values

# Feature selection with RandomForestClassifier
random_forest_classifier = RandomForestClassifier().fit(raw_features, labels)
feature_selector = SelectFromModel(random_forest_classifier, prefit=True)

# Transform the data to only contain the selected features and get the feature names
selected_features_data = feature_selector.transform(raw_features)
selected_features = malware_dataset.drop(['Name', 'md5', 'legitimate'], axis=1).columns[feature_selector.get_support()]

print(f"Selected features: {selected_features.tolist()}")
print(f"Original shape: {raw_features.shape}, Transformed shape: {selected_features_data.shape}")

# Save the selected features to a file
selected_features_path = 'selected_features1.pkl'
with open(selected_features_path, 'wb') as file:
    pickle.dump(selected_features, file)
print(f"Selected features saved to {selected_features_path}")

# Split the data into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(selected_features_data, labels, test_size=0.2, random_state=42, stratify=labels)

smote = SMOTE(random_state=42)
train_features_balanced, train_labels_balanced = smote.fit_resample(train_features, train_labels)

classifiers = {
    "Random Forest": RandomForestClassifier(class_weight="balanced"),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "Bagging": BaggingClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Naive Bayes": GaussianNB(),
    "XGBoost": xgb.XGBClassifier(),
    "LightGBM": LGBMClassifier(),
    "MLP": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500),
    "CatBoost": CatBoostClassifier(verbose=False),

}

results = []

# Train, evaluate, and collect metrics for each classifier
for name, clf in classifiers.items():
    print(f"Evaluating {name}...")

    # Cross-validation scores (apply CV on balanced data)
    cv_scores = cross_val_score(clf, train_features_balanced, train_labels_balanced, cv=5, scoring='accuracy')
    clf.fit(train_features_balanced, train_labels_balanced)

    # Test set predictions
    predictions = clf.predict(test_features)
    roc_auc = roc_auc_score(test_labels,
                            clf.predict_proba(test_features)[:, 1] if hasattr(clf, "predict_proba") else [0] * len(
                                test_labels))

    # Classification report for precision, recall, F1-score
    report = classification_report(test_labels, predictions, target_names=["Benign", "Malicious"])

    # Store metrics
    results.append({
        "Model": name,
        "Mean CV Accuracy": cv_scores.mean() * 100,
        "Test Accuracy": clf.score(test_features, test_labels) * 100,
        "ROC-AUC": roc_auc,
        "Classification Report": report,
    })

# Display results in a DataFrame
results_df = pd.DataFrame(results).sort_values(by="Test Accuracy", ascending=False)
print("\nModel Performance:")
print(results_df[["Model", "Mean CV Accuracy", "Test Accuracy", "ROC-AUC"]])

# Display detailed classification reports
for result in results:
    print(f"\nClassification Report for {result['Model']}:\n")
    print(result["Classification Report"])


"""
# Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 500, 700, 1000],
    'max_depth': [10, 20, 30, 40, 50, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', 0.3, 0.5, None],
}

# Create the Random Forest model
rf = RandomForestClassifier(class_weight="balanced", random_state=42)

# Randomized Search CV
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_grid,
    n_iter=50,  
    scoring='roc_auc',  
    cv=5,  
    verbose=2,
    random_state=42,
    n_jobs=-1  
)

# Fit the random search to the data
random_search.fit(train_features, train_labels)

# Best parameters and score
print("Best Parameters:", random_search.best_params_)
print("Best ROC-AUC Score:", random_search.best_score_)"""


# Best parameters from RandomizedSearchCV
best_params = {
    'n_estimators': 500,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': 0.3,
    'max_depth': 20
}

# Create tuned Random Forest model
tuned_random_forest_model = RandomForestClassifier(class_weight="balanced", random_state=42, **best_params)
tuned_random_forest_model.fit(train_features_balanced, train_labels_balanced)

# Cross-validation on balanced training data for Mean CV Accuracy
tuned_cv_scores = cross_val_score(tuned_random_forest_model, train_features_balanced, train_labels_balanced, cv=5, scoring='accuracy')

# Calculate Mean CV Accuracy
mean_cv_accuracy = tuned_cv_scores.mean() * 100

# Test Accuracy
test_accuracy = tuned_random_forest_model.score(test_features, test_labels) * 100

# ROC-AUC
roc_auc = roc_auc_score(test_labels, tuned_random_forest_model.predict_proba(test_features)[:, 1])

# Print the metrics
print("Tuned Model Metrics:")
print(f"Mean CV Accuracy: {mean_cv_accuracy:.2f}%")
print(f"Test Accuracy: {test_accuracy:.2f}%")
print(f"ROC-AUC Score: {roc_auc:.5f}")

tuned_predictions = tuned_random_forest_model.predict(test_features)
tuned_classification_report = classification_report(test_labels, tuned_predictions, target_names=["Benign", "Malicious"])
print(tuned_classification_report)





# Save the model
with open("tuned_random_forest_model1.pkl", 'wb') as f:
    pickle.dump(tuned_random_forest_model, f)
print("\nModel saved as 'tuned_random_forest_model1.pkl'")

