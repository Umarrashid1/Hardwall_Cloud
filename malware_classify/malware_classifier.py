import pandas as pd
import numpy as np
import pickle

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

# Additional imports for models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier



# Load the dataset
malware_dataset = pd.read_csv("/home/ubuntu/Desktop/P5-Hardwall/MalwareClassifier/MalwareData.csv", sep="|")

# Separate benign and malicious samples
benign_samples = malware_dataset[0:41323].drop(["legitimate"], axis=1)
malicious_samples = malware_dataset[41323::].drop(["legitimate"], axis=1)

print("Shape benign: %s samples, %s features" % (benign_samples.shape[0], benign_samples.shape[1]))
print("Shape malicious: %s samples, %s features" % (malicious_samples.shape[0], malicious_samples.shape[1]))

# Prepare features and labels
raw_features = malware_dataset.drop(['Name', 'md5', 'legitimate'], axis=1).values
labels = malware_dataset['legitimate'].values

# Feature selection with ExtraTreesClassifier
#extra_trees_classifier = ExtraTreesClassifier().fit(raw_features, labels)
#feature_selector = SelectFromModel(extra_trees_classifier, prefit=True)

# Feature selection with lightGBM
lightgbm_classifier = LGBMClassifier().fit(raw_features, labels)
feature_selector = SelectFromModel(lightgbm_classifier, prefit=True, threshold="mean")

# Transform the data with selected features
selected_features_data = feature_selector.transform(raw_features)
selected_features = malware_dataset.drop(['Name', 'md5', 'legitimate'], axis=1).columns[feature_selector.get_support()]

print(f"Selected features: {selected_features.tolist()}")
print(f"Original shape: {raw_features.shape}, Transformed shape: {selected_features_data.shape}")

# Save the selected features to a file
selected_features_path = 'selected_features.pkl'
with open(selected_features_path, 'wb') as file:
    pickle.dump(selected_features, file)
print(f"Selected features saved to {selected_features_path}")

# Split the data into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(selected_features_data, labels, test_size=0.2, random_state=42)

# Define a list of classifiers
classifiers = {
    "Random Forest": RandomForestClassifier(n_estimators=400),
    # "Gradient Boosting": GradientBoostingClassifier(n_estimators=100),
    # "AdaBoost": AdaBoostClassifier(n_estimators=100),
    # "Bagging": BaggingClassifier(n_estimators=100),
    # "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    # "Logistic Regression": LogisticRegression(max_iter=1000),
    # "Decision Tree": DecisionTreeClassifier(),
    # "Naive Bayes": GaussianNB(),
}

# Dictionary to store scores
model_scores = {}

# Train and evaluate each model
for name, clf in classifiers.items():
    print(f"Training {name}...")
    clf.fit(train_features, train_labels)
    score = clf.score(test_features, test_labels) * 100
    model_scores[name] = score
    print(f"{name} score: {score:.2f}%")

# Sort and display results
sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
print("\nModel performance comparison:")
for name, score in sorted_scores:
    print(f"{name}: {score:.2f}%")
