from flask import Flask, request, jsonify
import os
from predict import preprocess_continuous_data, model, scaler
import pandas as pd
from io import StringIO
import numpy as np

PORT = int(os.environ.get('PORT', 5300))
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100 MB limit

class StreamBuffer:
    def __init__(self, max_chunk_size):
        self.buffer = []
        self.max_chunk_size = max_chunk_size

    def add_data(self, new_data):
        self.buffer.extend(new_data)
        while len(self.buffer) >= self.max_chunk_size:
            yield self.buffer[:self.max_chunk_size]
            self.buffer = self.buffer[self.max_chunk_size:]

    def flush_buffer(self):
        if self.buffer:
            padded_chunk = np.zeros((self.max_chunk_size, len(self.buffer[0])))
            padded_chunk[:len(self.buffer), :] = self.buffer
            self.buffer = []
            return padded_chunk
        return None

buffer = StreamBuffer(max_chunk_size=10)

@app.route('/', methods=['POST'])
def analyze_keystrokes():
    data = request.data
    try:
        # Parse incoming JSON data
        df = pd.read_json(data)
        raw_features = df[['VK', 'HT', 'FT']].values
    except Exception:
        return jsonify({'error': 'Invalid data format'}), 400

    predictions = []
    # Process complete chunks using StreamBuffer
    for chunk in buffer.add_data(raw_features):
        # Use preprocess_continuous_data to scale data
        processed_chunk = preprocess_continuous_data(pd.DataFrame(chunk, columns=['VK', 'HT', 'FT']), scaler)
        predictions.append(model.predict(np.expand_dims(processed_chunk, axis=0)).flatten())

    # Handle any leftover data in the buffer
    final_chunk = buffer.flush_buffer()
    if final_chunk is not None:
        processed_chunk = preprocess_continuous_data(pd.DataFrame(final_chunk, columns=['VK', 'HT', 'FT']), scaler)
        predictions.append(model.predict(np.expand_dims(processed_chunk, axis=0)).flatten())

    # Combine predictions and return as binary labels
    predicted_labels = (np.concatenate(predictions) > 0.5).astype(int)
    return jsonify({'predictions': predicted_labels.tolist()})
