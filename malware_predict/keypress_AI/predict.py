import numpy as np
import pandas as pd
import joblib
from tf_keras.models import load_model

# Load the model and scaler
model = load_model('/home/ubuntu/hardwall/malware_predict/keypress_AI/sequential_model.keras')
scaler = joblib.load('/home/ubuntu/hardwall/malware_predict/keypress_AI/injection_scaler.pkl')

def preprocess_data(data, scaler, max_chunk_size=10):
    X = np.array([[entry['VK'], entry['HT'], entry['FT']] for entry in data])
    print("x:",X)

    # Separate VK from HT and FT
    X_vks = X[:, 0].reshape(-1, 1)  # VK values
    X_continuous = X[:, 1:]  # HT and FT columns

    # Scale HT and FT values
    X_continuous_scaled = scaler.transform(X_continuous)

    # Combine VKs with scaled HT and FT
    X_scaled = np.hstack((X_vks, X_continuous_scaled))

    # Group data into chunks
    def group_features(features, max_chunk_size):
        chunks = []
        current_chunk = []
        for feature in features:
            current_chunk.append(feature)
            if len(current_chunk) == max_chunk_size:
                chunks.append(current_chunk)
                current_chunk = []
        if current_chunk:
            padded_chunk = np.zeros((max_chunk_size, X_scaled.shape[1]))
            padded_chunk[:len(current_chunk), :] = current_chunk
            chunks.append(padded_chunk)
        return np.array(chunks)

    return group_features(X_scaled, max_chunk_size)

# Read input data from command-line arguments
input_data = json.loads(sys.argv[1])

# Preprocess the data
X_new_chunks = preprocess_data(input_data, scaler)

# Make predictions
predictions = model.predict(X_new_chunks).flatten()

predicted_labels = (predictions > 0.5).astype(int).tolist()

# Output predictions
print(json.dumps(predicted_labels))