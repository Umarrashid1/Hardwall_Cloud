#!/usr/bin/env python3
import sys
import json
import os
import warnings
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from tensorflow.keras.models import load_model

# Suppress TensorFlow logs
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
warnings.filterwarnings("ignore")

model_path = os.path.join(os.path.dirname(__file__), "sequential_model.keras")
scaler_path = os.path.join(os.path.dirname(__file__), "injection_scaler.pkl")

# Load model and scaler
try:
    model = load_model(model_path)
except ValueError as e:
    print(f"Error loading model: {e}")
    raise

scaler = joblib.load(scaler_path)

def group_features(features, max_chunk_size=10):
    """
    Groups feature arrays into chunks of size max_chunk_size,
    padding the last chunk if necessary.
    """
    chunks = []
    current_chunk = []

    for feature in features:
        current_chunk.append(feature)
        if len(current_chunk) == max_chunk_size:
            chunks.append(current_chunk)
            current_chunk = []

    # If there's a partial chunk, pad it
    if current_chunk:
        padded_chunk = np.zeros((max_chunk_size, features.shape[1]))
        padded_chunk[:len(current_chunk), :] = current_chunk
        chunks.append(padded_chunk)

    return np.array(chunks)

def preprocess_in_memory(data, scaler, max_chunk_size=10):
    """
    Expects `data` to be a list of dicts with keys ['VK', 'HT', 'FT'].
    Returns chunked and scaled Numpy arrays for model prediction.
    """
    # Convert to DataFrame
    df = pd.DataFrame(data)  # columns: 'VK', 'HT', 'FT'

    # X_vks = VK, X_continuous = [HT, FT]
    X = df[['VK', 'HT', 'FT']].values
    X_vks = X[:, 0].reshape(-1, 1)      # shape (n_samples, 1)
    X_continuous = X[:, 1:]            # shape (n_samples, 2) i.e. HT, FT

    # Scale the continuous columns (HT, FT)
    X_continuous_scaled = scaler.transform(X_continuous)

    # Concatenate scaled continuous data with the original VK as first column
    X_scaled = np.hstack((X_vks, X_continuous_scaled))

    # Group into chunks
    chunks = group_features(X_scaled, max_chunk_size)
    return chunks

if __name__ == "__main__":
    # 1) Read the data from stdin
    input_data = sys.stdin.read().strip()

    if not input_data:
        # If nothing was passed, exit gracefully (or throw an error if you want)
        print("[]")
        sys.exit(0)

    # 2) Parse the JSON (it should be a list of {VK, HT, FT} objects)
    data = json.loads(input_data)

    # 3) Preprocess and chunk
    X_new_chunks = preprocess_in_memory(data, scaler, max_chunk_size=10)

    if len(X_new_chunks) == 0:
        print("[]")
        sys.exit(0)

    # 4) Predict
    predictions = model.predict(X_new_chunks, verbose=0).flatten()
    predicted_labels = (predictions > 0.5).astype(int)

    # 5) Print out predictions as JSON
    print(json.dumps(predicted_labels.tolist()))
